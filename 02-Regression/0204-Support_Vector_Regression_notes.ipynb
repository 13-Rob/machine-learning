{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "---\n",
    "\n",
    "Support Vector Regression was invented in the 90s by Vladimir Vapnik and colleagues. Refer to *The Nature of Statistical Learning Theory* (1992).\n",
    "\n",
    "Similarly to linear regression, Support Vector Regression (SVR) trace a line that goes through our data, the main difference is the the SVR includes two other vectors, parallel to the main vector, but displaced by $ \\pm \\varepsilon $ on the vertical axis. This makes a tube called the $ \\varepsilon $-Insensitive Tube, and it's like a margin of error. If it falls within the $ \\varepsilon $-Intensitive Tube, we don't consider them as error, and whatever that fall out of the tube is considered as an error, and the distance is measured to the tube and not to the main line.\n",
    "\n",
    "## Support Vector Regression Equation\n",
    "\n",
    "$$ {1\\over{2}} \\Vert w \\Vert^2 + C \\sum_{i=1}^m (\\xi_{i} + \\xi_{i}^{*}) \\to min $$\n",
    "\n",
    "**Note**: The points outside of this \"tube\" are the support vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Reading\n",
    "\n",
    "*Chapter 4 - Support Vector Regression (from: Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers)*. Mariette Awad & Rahul Khanna (2015). Link: https://core.ac.uk/download/pdf/81523322.pdf."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
