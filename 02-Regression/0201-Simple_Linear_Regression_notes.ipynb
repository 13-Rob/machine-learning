{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Linear Regression Equation\n",
    "\n",
    "$$ \\hat{y} = b_{0} + b_{1} X_{1} $$\n",
    "\n",
    "In this equation, $\\hat{y}$ is the dependent variable, the values to be predicted; $X_{1}$ is the independent variable, the predictors; $b_{0}$, the y-intercept and constant; and $b_{1}$, the slope coefficient.\n",
    "\n",
    "## Ordinary Least Squares\n",
    "\n",
    "There are multiple lines that we can draw going through our data points, to determine the best linear regression we have the Ordinary Squares method.\n",
    "\n",
    "This methods projects every point from our data to the linear regression line. For each point in the graphic we have one in the linear regression line, $y_{i}$ and $\\hat{y}_{i}$. $y_{i}$ is the actual result, and $\\hat{y}_{i}$ is the value the linear regression predicts.\n",
    "\n",
    "Residual: $ \\varepsilon_{i} = y_{i} - \\hat{y}_i $\n",
    "\n",
    "We take the residual from every point in our data, and the best linear regression is that such as the sum of the square of the residuals is minimized.\n",
    "\n",
    "$ \\hat{y} = b_{0} + b_{1} X_{1} $ &emsp;&emsp; $ b_{0}, b_{1} | \\sum (y_{i} - \\hat{y}_{i})^2 $ is minimized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
