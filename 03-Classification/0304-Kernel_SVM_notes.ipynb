{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVM\n",
    "\n",
    "---\n",
    "\n",
    "There are multiple alternatives to the Support Vector when we have a set of data that is **Not Linearly Separable**.\n",
    "\n",
    "---\n",
    "\n",
    "## Mapping to a Higher Dimension\n",
    "\n",
    "We will take all our data and apply functions to transform our data, adding a new dimension so that it is possible to add a Maximum Margin Hyperplane that separates our data. The catch is that mapping to a higher dimension can be highly compute-intensive.\n",
    "\n",
    "**Mapping Function**\n",
    "\n",
    "$$ \\varphi (x_{1}, x_{2}) = (x_{1}, x_{2}, z) $$\n",
    "\n",
    "---\n",
    "\n",
    "## The Kernel Trick\n",
    "\n",
    "### Radial Basis Function (RBF) Kernel\n",
    "\n",
    "$$ K( \\overrightarrow{x}, \\overrightarrow{l^{i}} ) = e^{-{|| \\overrightarrow{x} - \\overrightarrow{l^i} ||}^2 \\over 2 \\sigma^2} $$\n",
    "\n",
    "Kernel ($K$) is a function applied to two vectors ($\\overrightarrow{x}$, and $\\overrightarrow{l^{i}}$). This allow us to create a \"*circle*\" around the landmark ($\\overrightarrow{l^{i}}$) to separate categories, instead of dividing categories with a single straight line. $ \\sigma $ represent the radius of this circle, also called decision boundary. You can even take multiple kernel functions and add them up. This way you can make a *non linear decision boundary*.\n",
    "\n",
    "$$ K(\\overrightarrow{x}, \\overrightarrow{l^{1}}) + K(\\overrightarrow{x}, \\overrightarrow{l^{2}}) $$\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Kernel Functions\n",
    "\n",
    "### Radial Basis Function (RBF) Kernel:\n",
    "\n",
    "$ K( \\overrightarrow{x}, \\overrightarrow{l^{i}} ) = e^{-{|| \\overrightarrow{x} - \\overrightarrow{l^i} ||}^2 \\over 2 \\sigma^2} $\n",
    "\n",
    "### Sigmoid Kernel\n",
    "\n",
    "$ K(X, Y) = tanh(\\gamma \\cdot X^{T} Y + r) $\n",
    "\n",
    "### Polynomial Kernel\n",
    "\n",
    "$ K(X, Y) = (\\gamma \\cdot X^{T} Y + r), \\gamma > 0 $\n",
    "\n",
    "### More Kernels\n",
    "\n",
    "https://juliapackages.com/p/kernelfunctions\n",
    "\n",
    "---\n",
    "\n",
    "## Non-Linear Kernel SVR (Advanced)\n",
    "\n",
    "The Kernel trick is used to \"plot\" our data in a 3-dimensional space, from there we trace a hyperplane that intersects our data, then the part where the hypar plane intersects the Kernel function is the marginfor our regression. Actually we don't have to go to a third dimension since we are using the kernel trick."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
