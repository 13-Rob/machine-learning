{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "PCA is not like regression, is not trying to predict values, is trying to understand the relationship between $X$ and $Y$ values.\n",
    "\n",
    "---\n",
    "\n",
    "## Uses of PCA\n",
    "\n",
    "- Noise filtering\n",
    "\n",
    "- Visualization\n",
    "\n",
    "- Feature Extraction\n",
    "\n",
    "- Stock market predictions\n",
    "\n",
    "- Gene data analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "Reduce the dimension of a d-dimensional dataset by projecting it onto a (k)-dimensional subspace (where k<d).\n",
    "\n",
    "- Identify patterns in data.\n",
    "\n",
    "- Detect the correlation between variables.\n",
    "\n",
    "---\n",
    "\n",
    "## PCA Algorithm\n",
    "\n",
    "Standarize the data.\n",
    "\n",
    "Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "\n",
    "Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$-largest eigenvalues where $k$ is the number of dimensions of the new feature subspace($k$<$d$).\n",
    "\n",
    "Construct the projection matrix **$W$** from the selected $k$ eigenvectors.\n",
    "\n",
    "Transform the original dataset **$X$** via **$W$** to obtain a $k$-dimensional feature subspace **$Y$**.\n",
    "\n",
    "### Eigenvector\n",
    "\n",
    "## Eigen Value\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Reading\n",
    "\n",
    "https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
