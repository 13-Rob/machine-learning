{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "PCA is not like regression, is not trying to predict values, is trying to understand the relationship between $X$ and $Y$ values.\n",
    "\n",
    "---\n",
    "\n",
    "## Uses of PCA\n",
    "\n",
    "- Noise filtering\n",
    "\n",
    "- Visualization\n",
    "\n",
    "- Feature Extraction\n",
    "\n",
    "- Stock market predictions\n",
    "\n",
    "- Gene data analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "Reduce the dimension of a d-dimensional dataset by projecting it onto a ($k$)-dimensional subspace (where $k < d$).\n",
    "\n",
    "- Identify patterns in data.\n",
    "\n",
    "- Detect the correlation between variables.\n",
    "\n",
    "---\n",
    "\n",
    "## PCA Algorithm\n",
    "\n",
    "**STEP 1**: Standarize the data.\n",
    "\n",
    "**STEP 2**: Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "\n",
    "**STEP 3**: Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$-largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k < d$).\n",
    "\n",
    "**STEP 4**: Construct the projection matrix **$W$** from the selected $k$ eigenvectors.\n",
    "\n",
    "**STEP 5**: Transform the original dataset **$X$** via **$W$** to obtain a $k$-dimensional feature subspace **$Y$**.\n",
    "\n",
    "### Eigenvector\n",
    "\n",
    "### Eigen Value\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Reading\n",
    "\n",
    "https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
